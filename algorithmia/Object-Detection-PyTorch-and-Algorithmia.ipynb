{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/determined/master/determined-logo.png\" align='right' width=150 />\n",
    "\n",
    "# Building a Pedestrian Detection Model with Determined\n",
    "\n",
    "<img src=\"https://www.cis.upenn.edu/~jshi/ped_html/images/PennPed00071_1.png\" width=400 />\n",
    "\n",
    "\n",
    "This notebook will walk through the benefits of building a Deep Learning model with Determined.  We will build an object detection model trained on the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) and deploy it to Algorithmia for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from determined.experimental import Determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the Determined cluster\n",
    "\n",
    "For our first example, we run a simple single-GPU training job with fixed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!det e create const.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determined Model Registry\n",
    "\n",
    "After training, we'll want to actually use our model in some sort of system.  Determined provides a model registry to version your trained models, making them easy to retrieve for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = < Experiment ID >\n",
    "MODEL_NAME = \"pedestrian_detection_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "checkpoint = Determined().get_experiment(experiment_id).top_checkpoint()\n",
    "model = Determined().create_model(MODEL_NAME)\n",
    "model.register_version(checkpoint.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Inference\n",
    "\n",
    "Once your model is versioned in the model registry, using that model for inference is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Determined().get_model(MODEL_NAME)\n",
    "trial = model.get_version().load()\n",
    "inference_model = trial.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import predict\n",
    "predict(inference_model, 'test.jpg', inference=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Endpoint\n",
    "\n",
    "Now that we can run inference in this notebook, let's set up a serving endpoint on Algorithmia so we can scale this model's serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll install Algorithmia and the associated utility packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Algorithmia\n",
    "!pip install retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algorithmia\n",
    "import algorithmia_utils\n",
    "import urllib.parse\n",
    "from git import Git, Repo, remote\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll next save the model from the Determined Model Registry to this current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inference_model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload your model and sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = <Your Algorithmia API Key>\n",
    "USERNAME = <Your Algorithmia username>\n",
    "\n",
    "client = Algorithmia.client(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLLECTION = \"pedestrian_detection\"\n",
    "DIRECTORY = client.dir(f\"data://{USERNAME}/{DATA_COLLECTION}\")\n",
    "if DIRECTORY.exists() is False:\n",
    "    DIRECTORY.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've created the data collection, you can upload your model and test image to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "MODEL_PATH = f\"data://{USERNAME}/{DATA_COLLECTION}/{MODEL_NAME}\"\n",
    "client.file(MODEL_PATH).putFile(MODEL_NAME)\n",
    "\n",
    "# Test image\n",
    "TEST_IMG_PATH = f\"data://{USERNAME}/{DATA_COLLECTION}/test.jpg\"\n",
    "client.file(TEST_IMG_PATH).putFile(\"test.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your serving Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithmia refers to each endpoint generically as an Algorithm. An Algorithm can be any executable code, in this case it is code that uses our model to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = \"pedestrian_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility = algorithmia_utils.AlgorithmiaUtils(API_KEY,\n",
    "                                                  USERNAME,\n",
    "                                                  ALGORITHM_NAME,\n",
    "                                                  local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility.create_algorithm(\"pytorch-1.5.x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Algorithm is created, we can add the inference code that we want it to execute. Algorithmia supports editing the Algorithm code in your preferred IDE via its Github repo. You an either create a repo within your own Github account (if you link your Github account to Algorithmia), or you can use the repo created for you automatically by Algorithmia. For this example, we'll do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility.clone_algorithm_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your algorithm repository comes with an initial script that is executed when you post your inference requests. We can edit and write that out to the cloned repo using the `%%writefile` macro. \n",
    "\n",
    "Make sure to edit the `model_path` variable in the script below, to point to the uploaded model path in your Algorithmia data source. Since we're creating the script with the %%writefile macro in this notebook, we won't be able to make use of our existing `DATA_COLLECTION` and `MODEL_PATH` variables to dynamically create this script, but will have to copy-paste their values into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $algo_utility.algo_script_path\n",
    "import Algorithmia\n",
    "import torch\n",
    "from .predict import predict\n",
    "\n",
    "#Defining our Algorithmia client in global scope\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def load_model(model_path):\n",
    "    \n",
    "    model_file = client.file(model_path).getFile().name\n",
    "    with open(model_file, 'rb') as model_file:\n",
    "        model = torch.load(model_file, map_location=torch.device('cuda'))\n",
    "        return model\n",
    "\n",
    "#Loading our model object in the global scope, outside the apply() function as we don't want do this operation every time our algorithm receives a request and be most efficient as possible\n",
    "model_path = \"data://.my/pedestrian_detection/pedestrian_detection_model\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "# API calls will begin at the apply() method, with the request body passed as 'input'\n",
    "# For more details, see algorithmia.com/developers/algorithm-development/languages\n",
    "def apply(input):\n",
    "    \n",
    "    img_path = input[\"img_path\"]\n",
    "    img = client.file(img_path).getFile().name\n",
    "    \n",
    "    with open(img, 'rb') as img:\n",
    "        predictions = predict(model, img, inference=\"algorithmia\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can create a dependency file to ensure the Algorithm builds the correct runtime environment.\n",
    "\n",
    "Since we created the algorithm on Algorithmia's Pytorch 1.15 GPU environment, we don't need to include the `torch` dependency in our requirements file. Our algorithm's base environment will be created with that installed and we will be building on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $algo_utility.dependency_file_path\n",
    "algorithmia>=1.0.0,<2.0\n",
    "torchvision==0.6.1\n",
    "pillow\n",
    "numpy\n",
    "matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the actual predictions, we can continue to use the `predict.py` and `data.py` modules that we used earlier. We'll now just copy it directly into the repo so that it can be uploaded to the Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp predict.py ./{ALGORITHM_NAME}/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to push the updated repo to Algorithmia.\n",
    "\n",
    "This operation will trigger the algorithm build at Algorithmia's git server. So you should wait until you see the \"Build succesful\" message in the output of the next cell, before you continue with sending your inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility.push_algo_script_with_dependencies(filenames=[\n",
    "    f\"{ALGORITHM_NAME}.py\",\n",
    "    \"predict.py\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Algorithm created and inference code pushed, we can run inference on the test image we uploaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_result = algo_utility.call_latest_algo_version({\n",
    "    \"img_path\": TEST_IMG_PATH\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo_result.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo_result.result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
